{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imaKj7J2vkj5",
        "outputId": "6843e07a-141c-4351-b767-76cfaf25f707"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from utils import process_tweet, lookup\n",
        "import pdb\n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from os import getcwd\n",
        "\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "test_pos = all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "\n",
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg\n",
        "\n",
        "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
        "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
      ],
      "metadata": {
        "id": "deYs8rwSvq2V"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
        "print(process_tweet(custom_tweet))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TupSps7wsY8",
        "outputId": "9e2daeaa-f7c5-44b9-b2d4-152835e6d36a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'great', 'day', ':)', 'good', 'morn']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_tweets(result, tweets, ys):\n",
        "    for y, tweet in zip(ys, tweets):\n",
        "        for word in process_tweet(tweet):\n",
        "            pair = (word,y)\n",
        "            if pair in result:\n",
        "                result[pair] += 1\n",
        "            else:\n",
        "                result[pair] = 1\n",
        "    return result"
      ],
      "metadata": {
        "id": "1oAMsJmcw7bA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = {}\n",
        "tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n",
        "ys = [1, 0, 0, 0, 0]\n",
        "count_tweets(result, tweets, ys)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiR3Lu6BxNSQ",
        "outputId": "0173963b-e354-4ed3-d37b-d7345685ef61"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('happi', 1): 1, ('trick', 0): 1, ('sad', 0): 1, ('tire', 0): 2}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_naive_bayes(freqs, train_x, train_y):\n",
        "    loglikelihood = {}\n",
        "    logprior = 0\n",
        "    vocab = set([pair[0] for pair in freqs.keys()])\n",
        "    V = len(vocab)\n",
        "    N_pos = N_neg = V_pos = V_neg = 0\n",
        "    for pair in freqs.keys():\n",
        "        if pair[1] > 0:\n",
        "            V_pos += 1\n",
        "            N_pos += freqs[pair]\n",
        "        else:\n",
        "            V_neg += 1\n",
        "            N_neg += freqs[pair]\n",
        "    D = len(train_y)\n",
        "    D_pos = (len(list(filter(lambda x: x > 0, train_y))))\n",
        "    D_neg = (len(list(filter(lambda x: x <= 0, train_y))))\n",
        "    logprior = np.log(D_pos) - np.log(D_neg)\n",
        "    for word in vocab:\n",
        "        freq_pos = lookup(freqs,word,1)\n",
        "        freq_neg = lookup(freqs,word,0)\n",
        "        p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
        "        p_w_neg = (freq_neg + 1) / (N_neg + V)\n",
        "        loglikelihood[word] = np.log(p_w_pos/p_w_neg)\n",
        "\n",
        "    return logprior, loglikelihood"
      ],
      "metadata": {
        "id": "wfBKAuXuxUCl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freqs = count_tweets({}, train_x, train_y)"
      ],
      "metadata": {
        "id": "6vutj8zMyHsZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n",
        "print(logprior)\n",
        "print(len(loglikelihood))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6XmDb-uxxpB",
        "outputId": "480145df-9ac9-478c-c99a-a59b7886274f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n",
            "9161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
        "    word_l = process_tweet(tweet)\n",
        "    p = 0\n",
        "    p += logprior\n",
        "    for word in word_l:\n",
        "        if word in loglikelihood:\n",
        "            p += loglikelihood[word]\n",
        "    return p"
      ],
      "metadata": {
        "id": "v5e9Q7cKx2Ej"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_tweet = 'She smiled.'\n",
        "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
        "print('The expected output is', p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArgoJNyjyY0q",
        "outputId": "79c84cfa-5ab9-4dd4-da0f-43b8488d048c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The expected output is 1.557492820301094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
        "    accuracy = 0\n",
        "    y_hats = []\n",
        "    for tweet in test_x:\n",
        "        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
        "            y_hat_i = 1\n",
        "        else:\n",
        "            y_hat_i = 0\n",
        "        y_hats.append(y_hat_i)\n",
        "    error = np.mean(np.absolute(y_hats-test_y))\n",
        "    accuracy = 1-error\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "xmt4PY6hyijF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Naive Bayes accuracy = %0.4f\" %\n",
        "      (test_naive_bayes(test_x, test_y, logprior, loglikelihood)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uj8AvGMby3pw",
        "outputId": "dc9a1dc2-9d4d-42a1-c84b-9fb0c00e98e6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes accuracy = 0.9955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ratio(freqs, word):\n",
        "    pos_neg_ratio = {'positive': 0, 'negative': 0, 'ratio': 0.0}\n",
        "    pos_neg_ratio['positive'] = lookup(freqs,word,1)\n",
        "    pos_neg_ratio['negative'] = lookup(freqs,word,0)\n",
        "    pos_neg_ratio['ratio'] = (pos_neg_ratio['positive'] + 1)/(pos_neg_ratio['negative'] + 1)\n",
        "    return pos_neg_ratio"
      ],
      "metadata": {
        "id": "mtGnJJLyy8L4"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_ratio(freqs, 'happi')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NH_XdLbzPND",
        "outputId": "9d3f6709-a07f-460f-e784-0fc63c7f84d6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'positive': 162, 'negative': 18, 'ratio': 8.578947368421053}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_words_by_threshold(freqs, label, threshold):\n",
        "    word_list = {}\n",
        "    for key in freqs.keys():\n",
        "        word, _ = key\n",
        "        pos_neg_ratio = get_ratio(freqs, word)\n",
        "        if label == 1 and pos_neg_ratio['ratio'] >= threshold :\n",
        "            word_list[word] = pos_neg_ratio\n",
        "        elif label == 0 and pos_neg_ratio['ratio'] <= threshold:\n",
        "            word_list[word] = pos_neg_ratio\n",
        "    return word_list"
      ],
      "metadata": {
        "id": "-bd7tmmBzSKp"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_words_by_threshold(freqs, label=0, threshold=0.05)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSHyslgiziKb",
        "outputId": "9f386536-83e7-42a7-c0b5-3a661d762896"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{':(': {'positive': 1, 'negative': 3675, 'ratio': 0.000544069640914037},\n",
              " ':-(': {'positive': 0, 'negative': 386, 'ratio': 0.002583979328165375},\n",
              " 'zayniscomingbackonjuli': {'positive': 0, 'negative': 19, 'ratio': 0.05},\n",
              " '26': {'positive': 0, 'negative': 20, 'ratio': 0.047619047619047616},\n",
              " '>:(': {'positive': 0, 'negative': 43, 'ratio': 0.022727272727272728},\n",
              " 'lost': {'positive': 0, 'negative': 19, 'ratio': 0.05},\n",
              " '♛': {'positive': 0, 'negative': 210, 'ratio': 0.004739336492890996},\n",
              " '》': {'positive': 0, 'negative': 210, 'ratio': 0.004739336492890996},\n",
              " 'beli̇ev': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
              " 'wi̇ll': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
              " 'justi̇n': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
              " 'ｓｅｅ': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
              " 'ｍｅ': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776}}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_words_by_threshold(freqs, label=1, threshold=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98h5ybJXzkz1",
        "outputId": "266790dc-4ce5-49b2-8c56-6bebc1940570"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'followfriday': {'positive': 23, 'negative': 0, 'ratio': 24.0},\n",
              " 'commun': {'positive': 27, 'negative': 1, 'ratio': 14.0},\n",
              " ':)': {'positive': 2960, 'negative': 2, 'ratio': 987.0},\n",
              " 'flipkartfashionfriday': {'positive': 16, 'negative': 0, 'ratio': 17.0},\n",
              " ':d': {'positive': 523, 'negative': 0, 'ratio': 524.0},\n",
              " ':p': {'positive': 105, 'negative': 0, 'ratio': 106.0},\n",
              " 'influenc': {'positive': 16, 'negative': 0, 'ratio': 17.0},\n",
              " ':-)': {'positive': 552, 'negative': 0, 'ratio': 553.0},\n",
              " \"here'\": {'positive': 20, 'negative': 0, 'ratio': 21.0},\n",
              " 'youth': {'positive': 14, 'negative': 0, 'ratio': 15.0},\n",
              " 'bam': {'positive': 44, 'negative': 0, 'ratio': 45.0},\n",
              " 'warsaw': {'positive': 44, 'negative': 0, 'ratio': 45.0},\n",
              " 'shout': {'positive': 11, 'negative': 0, 'ratio': 12.0},\n",
              " ';)': {'positive': 22, 'negative': 0, 'ratio': 23.0},\n",
              " 'stat': {'positive': 51, 'negative': 0, 'ratio': 52.0},\n",
              " 'arriv': {'positive': 57, 'negative': 4, 'ratio': 11.6},\n",
              " 'glad': {'positive': 41, 'negative': 2, 'ratio': 14.0},\n",
              " 'blog': {'positive': 27, 'negative': 0, 'ratio': 28.0},\n",
              " 'fav': {'positive': 11, 'negative': 0, 'ratio': 12.0},\n",
              " 'fantast': {'positive': 9, 'negative': 0, 'ratio': 10.0},\n",
              " 'fback': {'positive': 26, 'negative': 0, 'ratio': 27.0},\n",
              " 'pleasur': {'positive': 10, 'negative': 0, 'ratio': 11.0},\n",
              " '←': {'positive': 9, 'negative': 0, 'ratio': 10.0},\n",
              " 'aqui': {'positive': 9, 'negative': 0, 'ratio': 10.0}}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_tweet = 'I am happy because I am learning :)'\n",
        "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
        "print(p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIRSUnK_zqCD",
        "outputId": "afe723f4-8065-4a07-8709-cc8adacd41ce"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9.570227756170972\n"
          ]
        }
      ]
    }
  ]
}